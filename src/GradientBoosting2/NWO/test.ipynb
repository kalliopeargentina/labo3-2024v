{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20132\\638593639.py\", line 4, in <module>\n",
      "    import labolibrary as labo\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\src\\GradientBoosting2\\NWO\\labolibrary.py\", line 5, in <module>\n",
      "    import dtaidistance as dtw\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dtaidistance\\__init__.py\", line 19, in <module>\n",
      "    from . import dtw\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dtaidistance\\dtw.py\", line 17, in <module>\n",
      "    from . import ed\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dtaidistance\\ed.py\", line 16, in <module>\n",
      "    from . import innerdistance\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dtaidistance\\innerdistance.py\", line 16, in <module>\n",
      "    from . import util\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dtaidistance\\util.py\", line 41, in <module>\n",
      "    from . import dtw_cc_numpy\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20132\\638593639.py\", line 4, in <module>\n",
      "    import labolibrary as labo\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\src\\GradientBoosting2\\NWO\\labolibrary.py\", line 5, in <module>\n",
      "    import dtaidistance as dtw\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dtaidistance\\__init__.py\", line 19, in <module>\n",
      "    from . import dtw\n",
      "  File \"d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dtaidistance\\dtw.py\", line 46, in <module>\n",
      "    from . import dtw_cc_numpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81641\n",
      "[LightGBM] [Info] Number of data points in the train set: 257834, number of used features: 804\n",
      "[LightGBM] [Info] Start training from score 0.000064\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalidation's multinacional_metric: 0.222672\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.540121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 81660\n",
      "[LightGBM] [Info] Number of data points in the train set: 515667, number of used features: 814\n",
      "[LightGBM] [Info] Start training from score 0.000038\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalidation's multinacional_metric: 0.102019\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.985639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 116834\n",
      "[LightGBM] [Info] Number of data points in the train set: 773500, number of used features: 991\n",
      "[LightGBM] [Info] Start training from score 0.000028\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's multinacional_metric: 0.0133087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 6.322042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117317\n",
      "[LightGBM] [Info] Number of data points in the train set: 1031333, number of used features: 1017\n",
      "[LightGBM] [Info] Start training from score 0.000027\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalidation's multinacional_metric: 0.00238259\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.543308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 117428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1289166, number of used features: 1053\n",
      "[LightGBM] [Info] Start training from score 0.000026\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's multinacional_metric: 0.789396\n",
      "Overall custom metric:  0.0023825623529702446\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import labolibrary as labo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "DATOS_DIR = 'D:/Dropbox/Python/LaboIII/labo3-2024v/src/GradientBoosting2/data/'\n",
    "\n",
    "# Definir la métrica personalizada\n",
    "def multinacional_metric(y_true, y_pred):\n",
    "    return abs(sum(y_true - y_pred)) / sum(y_true)\n",
    "\n",
    "# Función para escalar y devolver una serie\n",
    "def minmax_scale_group(group):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_values = scaler.fit_transform(group.values.reshape(-1, 1)).flatten()\n",
    "    scalers[group.name] = scaler  # Almacenar el escalador para este grupo\n",
    "    return pd.Series(scaled_values, index=group.index)\n",
    "\n",
    "# Función para desescalar y devolver una serie\n",
    "def inverse_minmax_scale_group(group):\n",
    "    scaler = scalers[group.name]\n",
    "    inversed_values = scaler.inverse_transform(group.values.reshape(-1, 1)).flatten()\n",
    "    return pd.Series(inversed_values, index=group.index)\n",
    "\n",
    "# Leer datos\n",
    "df_final = pd.read_parquet(DATOS_DIR+'FE_dataset-CARLA.parquet') \n",
    "df_final.columns = df_final.columns.str.replace(' ', '_').str.replace(r'[^A-Za-z0-9_]', '', regex=True)\n",
    "\n",
    "### Filtrar datos\n",
    "df_true = df_final.loc['2019-12-01':'2020-02-01']\n",
    "df_final = df_final.loc['2018-01-01':'2020-01-01']\n",
    "\n",
    "### Indices\n",
    "df_final.index = df_final.index.to_timestamp()\n",
    "df_true.index = df_true.index.to_timestamp()\n",
    "\n",
    "### Agrupar y escalar\n",
    "scalers = {}\n",
    "\n",
    "# Create the feature that represents the difference between TN+2 and TN\n",
    "df_final['TN'] = df_final['tn']\n",
    "df_final['TN+2'] = df_final.groupby('product_id')['tn'].shift(-2)\n",
    "df_final['diff2'] = df_final['TN+2'] - df_final['TN']\n",
    "\n",
    "# Normalize TN and TN+2\n",
    "df_final['TN_normalized'] = df_final.groupby('product_id')['TN'].transform(minmax_scale_group)\n",
    "df_final['TN+2_normalized'] = df_final.groupby('product_id')['TN+2'].transform(minmax_scale_group)\n",
    "\n",
    "# Create the normalized difference feature\n",
    "df_final['diff2_normalized'] = df_final['TN+2_normalized'] - df_final['TN_normalized']\n",
    "\n",
    "# Correr Modelo\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'None',\n",
    "}\n",
    "model, average_metric = labo.train_lightgbm_model(df_final, params, col='diff2_normalized')\n",
    "print(\"Overall custom metric: \", average_metric)\n",
    "\n",
    "def predict_next_month(model, last_data_points, col='diff2_normalized'):\n",
    "    predictions = []\n",
    "    last_month = last_data_points.index.max() + pd.DateOffset(months=2)  # Predict 2 months ahead\n",
    "    last_data_points.index = [last_month] * len(last_data_points)\n",
    "    \n",
    "    predictions = model.predict(last_data_points, num_iteration=model.best_iteration,\n",
    "                                predict_disable_shape_check=True)\n",
    "    \n",
    "    prediction_df = last_data_points[['product_id', 'TN_normalized']].copy()\n",
    "    prediction_df['diff2_normalized'] = predictions\n",
    "    prediction_df.index = [last_month] * len(last_data_points)\n",
    "\n",
    "    return prediction_df\n",
    "\n",
    "def prepare_data_for_prediction(df, last_date):\n",
    "    last_data = df[df.index <= last_date].copy()\n",
    "    last_data['TN+2'] = np.nan  # Set TN+2 to NaN for the last two months\n",
    "    last_data['diff2'] = np.nan  # Set diff2 to NaN for the last two months\n",
    "    \n",
    "    # Normalize TN and create other necessary features\n",
    "    last_data['TN_normalized'] = last_data.groupby('product_id')['TN'].transform(minmax_scale_group)\n",
    "    \n",
    "    return last_data\n",
    "\n",
    "def update_dataset_with_prediction(df, predictions, prediction_date):\n",
    "    for idx, row in predictions.iterrows():\n",
    "        df.loc[(df.index == prediction_date) & (df['product_id'] == row['product_id']), 'TN'] = row['TN+2']\n",
    "        df.loc[(df.index == prediction_date) & (df['product_id'] == row['product_id']), 'TN_normalized'] = row['TN+2_normalized']\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for 2019-12-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Predict for December, January, and February\n",
    "months_to_predict = ['2019-12-01', '2020-01-01', '2020-02-01']\n",
    "all_predictions = []\n",
    "\n",
    "for i, predict_month in enumerate(months_to_predict):\n",
    "    print(f\"Predicting for {predict_month}\")\n",
    "    \n",
    "    # Prepare data for prediction\n",
    "    last_date = pd.to_datetime(predict_month) - pd.DateOffset(months=2)\n",
    "    last_data_points = prepare_data_for_prediction(df_final, last_date)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = predict_next_month(model, last_data_points)\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    predictions['TN+2_normalized'] = predictions['TN_normalized'] + predictions['diff2_normalized']\n",
    "    predictions['TN+2'] = predictions.groupby('product_id')['TN+2_normalized'].transform(inverse_minmax_scale_group)\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions['prediction_date'] = predict_month\n",
    "    all_predictions.append(predictions)\n",
    "    \n",
    "    # Update dataset with new prediction for next iteration\n",
    "    df_final = update_dataset_with_prediction(df_final, predictions, pd.to_datetime(predict_month))\n",
    "\n",
    "# Combine all predictions\n",
    "final_predictions = pd.concat(all_predictions)\n",
    "\n",
    "# Get February predictions\n",
    "february_predictions = final_predictions[final_predictions['prediction_date'] == '2020-02-01']\n",
    "february_predictions = february_predictions.groupby('product_id')['TN+2'].sum().reset_index()\n",
    "february_predictions.columns = ['product_id', 'tn']\n",
    "\n",
    "# Save predictions\n",
    "february_predictions.to_csv(DATOS_DIR+'/pred/predicciones-v1.csv', index=False, header=True)\n",
    "\n",
    "print(\"Predictions for February saved.\")\n",
    "\n",
    "# Calculate error for February\n",
    "if '2020-02-01' in df_true.index:\n",
    "    true_values = df_true.loc['2020-02-01'].groupby('product_id')['tn'].sum()\n",
    "    predicted_values = february_predictions.set_index('product_id')['tn']\n",
    "    error = abs(sum(true_values - predicted_values)) / sum(true_values)\n",
    "    print(\"Error for February: \", error)\n",
    "\n",
    "# Calculate overall error for December, January, and February\n",
    "all_true_values = df_true.groupby('product_id')['tn'].sum()\n",
    "all_predicted_values = final_predictions.groupby('product_id')['TN+2'].sum()\n",
    "overall_error = abs(sum(all_true_values - all_predicted_values)) / sum(all_true_values)\n",
    "print(\"Overall Error for Dec, Jan, Feb: \", overall_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
