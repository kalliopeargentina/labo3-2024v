{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import labolibrary as labo\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DATOS_DIR = '~/buckets/b1/datasets/'\n",
    "DATOS_DIR = '../data/'      \n",
    "\n",
    "scalers = {}\n",
    "# Function to center, scale, and return a series\n",
    "def scale_group(group):\n",
    "    scaler = RobustScaler()\n",
    "    scaled_values = scaler.fit_transform(group.values.reshape(-1, 1)).flatten()\n",
    "    scalers[group.name] = scaler  # Store the scaler for this group\n",
    "    return pd.Series(scaled_values, index=group.index, name=group.name)\n",
    "\n",
    "# Function to inverse transform (de-scale) and decenter, and return a series\n",
    "def inverse_scale_group(group):\n",
    "    group_name = group.name\n",
    "    scaler = scalers[group_name]\n",
    "    inversed_centered_values = scaler.inverse_transform(group.values.reshape(-1, 1)).flatten()\n",
    "    original_values = inversed_centered_values\n",
    "    return pd.Series(original_values, index=group.index, name=group_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Leer datos\n",
    "#df_final = pd.read_parquet(DATOS_DIR+'FE_dataset-CARLA.parquet') \n",
    "df_final = pd.read_parquet(DATOS_DIR+'/FE_02_dataset.parquet') \n",
    "df_final.columns = df_final.columns.str.replace(' ', '_').str.replace(r'[^A-Za-z0-9_]', '', regex=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = df_final.groupby(['product_id', df_final.index])['weight'].transform('mean')\n",
    "weights = columns=['weight']\n",
    "df_final.groupby('product_id')['weight'].transform(scale_group)\n",
    "df_final.drop(columns=['weight'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Filtrar datos\n",
    "df_true = df_final.loc['2019-12-01':'2020-01-01']\n",
    "df_final = df_final.loc['2018-01-01':'2019-11-01']\n",
    "\n",
    "\n",
    "#Filtro de no compradores\n",
    "#Step 1: Ensure the index is a datetime type\n",
    "df_final.index = df_final.index.to_timestamp()\n",
    "# Step 2: Determine the last date and calculate the date 3 months prior\n",
    "ls_date  = df_final.index.max()\n",
    "three_months_prior = ls_date - pd.DateOffset(months=3)\n",
    "\n",
    "# Step 3: Filter the dataframe to include onl+y rows within the last 3 months\n",
    "last_3_months_df = df_final[df_final.index >= three_months_prior]\n",
    "\n",
    "# Step 4: Identify the unique client_id that have purchased within this period\n",
    "active_clients = last_3_months_df['customer_id'].unique()\n",
    "\n",
    "# Step 5: Filter the original dataframe to include only these client_id\n",
    "df_final = df_final[df_final['customer_id'].isin(active_clients)]\n",
    "\n",
    "df_final.index = pd.PeriodIndex(df_final.index, freq='M')\n",
    "\n",
    "\n",
    "#Filtro test\n",
    "df_final = df_final[df_final['product_id'] < 20013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and predicting for product_id: 20007\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714us/step - loss: -0.8909\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: -8.4420\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: -14.5537\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: -23.9662\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: -33.1004\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: -41.4282\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: -46.3978\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: -58.7956\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: -64.6594\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: -63.9414\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_lightgbm_model() got an unexpected keyword argument 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m df_producto[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtn_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#############################################################################  \u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#### Agrupar y escalar\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m model, average_metric \u001b[38;5;241m=\u001b[39m \u001b[43mlabo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_lightgbm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_producto\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall rmse metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m, average_metric)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Predict values for the entire dataset using the trained models\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Prepare last data points for prediction\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_lightgbm_model() got an unexpected keyword argument 'weights'"
     ]
    }
   ],
   "source": [
    "# Correr Modelo\n",
    "params={\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'Regression',\n",
    "        'metric':'rmse',\n",
    "        'verbose': -1,\n",
    "        #'n_jobs': -1,\n",
    "        #'seed': 113,\n",
    "        #'learning_rate': 0.2,\n",
    "        #'bagging_fraction': 0.85,\n",
    "        #'bagging_freq': 1, \n",
    "        #'colsample_bytree': 0.85,\n",
    "        #'colsample_bynode': 0.85,\n",
    "        #'min_data_per_leaf': 25,\n",
    "        #'num_leaves': 200,\n",
    "        #'lambda_l1': 0.5,\n",
    "        #'lambda_l2': 0.5\n",
    "}\n",
    "\n",
    "predictions_all = pd.DataFrame(columns=['tn'])\n",
    "products = df_final['product_id'].unique()\n",
    "tot = len(products)\n",
    "nro = 0\n",
    "for producto in products:\n",
    "    print(f'Fitting and predicting for product_id: {producto}')\n",
    "    # Filtrar los datos del producto\n",
    "    df_producto = df_final[df_final['product_id'] == producto]\n",
    "\n",
    "    # Prepare data for LSTM on tn_2 only\n",
    "    X = df_producto[['tn_2']].values.astype('float32')\n",
    "    y = df_producto['tn_2'].values.astype('float32')\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "    #######################################################    \n",
    "    # Define LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    # Train LSTM model\n",
    "    model.fit(X, y, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "    # Extract features from LSTM\n",
    "    features = model.predict(X)\n",
    "\n",
    "    # Prepare data for LightGBM\n",
    "    # Convert LSTM features to DataFrame\n",
    "    features_df = pd.DataFrame(features, index=df_producto.index, columns=[f'lstm_feature_{i}' for i in range(features.shape[1])])\n",
    "    df_producto = pd.concat([df_producto, features_df], axis=1)\n",
    "    df_producto['tn_2'] = y\n",
    "    #############################################################################  \n",
    "    #### Agrupar y escalar\n",
    "   \n",
    "    model, average_metric = labo.train_lightgbm_model(df_producto,params,metric='rmse')\n",
    "    print(\"Overall rmse metric: \", average_metric)\n",
    "    # Predict values for the entire dataset using the trained models\n",
    "    # Prepare last data points for prediction\n",
    "    last_data_points = df_producto[df_producto.index == df_producto.index.max()].copy()\n",
    "    last_data_points.drop(columns=['tn_2'], inplace=True)\n",
    "    # Predict the next month's value using the trained model\n",
    "    predictions = labo.predict_next_month(model, last_data_points)\n",
    "    preds = predictions.groupby('product_id')['tn_2'].transform(inverse_scale_group)\n",
    "    predictions['tn'] = preds\n",
    "    predictions.drop(columns=['tn_2'], inplace=True)\n",
    "    predictions = predictions.reset_index()\n",
    "    predictions =  predictions.groupby('product_id')['tn'].sum()\n",
    "    predictions.columns = ['product_id', 'tn']\n",
    "    predictions_all = pd.concat([predictions_all, predictions])\n",
    "    print(predictions_all[-1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall custom metric:  3.57567022685441\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_all['tn']=predictions_all['tn'].astype('float32')\n",
    "predictions_all.index.names = ['product_id']\n",
    "predictions_all.to_csv(DATOS_DIR+'/pred/0018-prediccion-rmse_scaled_CORRECT2-product_id_LSTM-No_Buyer.csv', index=True,header=True)\n",
    "print(\"Overall custom metric: \", average_metric)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
