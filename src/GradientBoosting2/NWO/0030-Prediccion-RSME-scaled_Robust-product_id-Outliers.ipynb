{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dropbox\\Python\\LaboIII\\labo3-2024v\\.venv\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import labolibrary as labo\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler,PowerTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DATOS_DIR = '~/buckets/b1/datasets/'\n",
    "DATOS_DIR = '../data/'      \n",
    "\n",
    "scalers = {}\n",
    "# Function to center, scale, and return a series\n",
    "def scale_group(group):\n",
    "    scaler = RobustScaler()\n",
    "    #scaler = PowerTransformer()\n",
    "    scaled_values = scaler.fit_transform(group.values.reshape(-1, 1)).flatten()\n",
    "    scalers[group.name] = scaler  # Store the scaler for this group\n",
    "    return pd.Series(scaled_values, index=group.index, name=group.name)\n",
    "\n",
    "# Function to inverse transform (de-scale) and decenter, and return a series\n",
    "def inverse_scale_group(group):\n",
    "    group_name = group.name\n",
    "    scaler = scalers[group_name]\n",
    "    inversed_centered_values = scaler.inverse_transform(group.values.reshape(-1, 1)).flatten()\n",
    "    original_values = inversed_centered_values\n",
    "    return pd.Series(original_values, index=group.index, name=group_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Leer datos\n",
    "#df_final = pd.read_parquet(DATOS_DIR+'FE_dataset-CARLA.parquet') \n",
    "#import dask.dataframe as dd\n",
    "df_final = pd.read_parquet(DATOS_DIR+'FE_07_dataset.parquet') \n",
    "#df_final = df_final.compute()\n",
    "\n",
    "df_final.columns = df_final.columns.str.replace(' ', '_').str.replace(r'[^A-Za-z0-9_]', '', regex=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ESTO ES UN HACK PORQUE ESTA MAL ARMADA LA FUNCIÓN DE ESCALADO Y SOLO GUARDA LOS DATOS DE LA ÜLTIMA APLICACIÓN\n",
    "\n",
    "with open(DATOS_DIR + '/scalers.pkl', 'rb') as file:\n",
    "    scalers = pickle.load(file)\n",
    "\n",
    "#df_test = df_final[(df_final['product_id'] == 21276) & (df_final['customer_id'] == 10029)][['product_id','tn', 'weight']]\n",
    "#df_test['tn'] = df_test.groupby('product_id')['tn'].transform(inverse_scale_group)\n",
    "\n",
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Filtrar datos\n",
    "df_true = df_final.loc['2019-12-01':'2020-01-01']\n",
    "df_final = df_final.loc['2018-01-01':'2019-10-01']\n",
    "df_predict = df_final.loc['2018-01-01':'2019-10-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>periodo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20307</td>\n",
       "      <td>10417</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20320</td>\n",
       "      <td>10566</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20600</td>\n",
       "      <td>10423</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20322</td>\n",
       "      <td>10318</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20296</td>\n",
       "      <td>10027</td>\n",
       "      <td>0.076756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20203</td>\n",
       "      <td>10026</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20203</td>\n",
       "      <td>10027</td>\n",
       "      <td>0.876033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20680</td>\n",
       "      <td>10348</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>20203</td>\n",
       "      <td>10028</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12</th>\n",
       "      <td>21276</td>\n",
       "      <td>10550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156630 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         product_id  customer_id        tn\n",
       "periodo                                   \n",
       "2019-12       20307        10417  0.000000\n",
       "2019-12       20320        10566  0.000000\n",
       "2019-12       20600        10423  0.000000\n",
       "2019-12       20322        10318  0.000000\n",
       "2019-12       20296        10027  0.076756\n",
       "...             ...          ...       ...\n",
       "2019-12       20203        10026  0.000000\n",
       "2019-12       20203        10027  0.876033\n",
       "2019-12       20680        10348  0.000000\n",
       "2019-12       20203        10028  0.000000\n",
       "2019-12       21276        10550  0.000000\n",
       "\n",
       "[156630 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = df_true[['product_id','customer_id','tn']]\n",
    "df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true['tn'] = df_true.groupby(['product_id','customer_id'])['tn'].transform(inverse_scale_group)\n",
    "pred_true = df_true.groupby(['product_id'])['tn'].sum()\n",
    "pred_true.to_csv(DATOS_DIR+'/pred/TRUE-NOVEMBER.csv', index=True,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final[df_final['product_id'] == 20402 & df_final['customer_id'] == 10479][['product_id','tn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Filtro test\n",
    "#df_final = df_final[df_final['product_id'] < 20013]\n",
    "#df_final = df_final[df_final['product_id'] == 20072]\n",
    "\n",
    "weight= df_final[['weight','product_id','customer_id']]\n",
    "#df_final.drop(columns=['weight'], inplace=True)\n",
    "#filtered_weight = weight[weight['weight'] != 0]\n",
    "#weight_mean = filtered_weight.groupby('product_id')['weight'].mean().to_dict()\n",
    "#weight['weight'] = weight['product_id'].map(weight_mean)\n",
    "\n",
    "weight = weight.groupby(['product_id','customer_id'])['weight'].mean()\n",
    "weight = weight.reset_index()\n",
    "df_final = df_final.drop(columns=['weight'])\n",
    "df_predict = df_predict.drop(columns=['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and predicting for product_id: 20003\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalidation's rmse: 0.68186\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalidation's rmse: 0.943687\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalidation's rmse: 0.573478\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalidation's rmse: 0.543427\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalidation's rmse: 0.875818\n",
      "Overall rmse metric:  0.29531322446228425\n",
      "               tn\n",
      "20003  875.297307\n",
      "Fitting and predicting for product_id: 20012\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2144\\90720821.py:52: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  predictions_all = pd.concat([predictions_all, predictions])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2144\\90720821.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictions_all = pd.concat([predictions_all, predictions])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[332]\tvalidation's rmse: 3.45767\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalidation's rmse: 2.12069\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalidation's rmse: 1.44472\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalidation's rmse: 4.01764\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalidation's rmse: 1.12664\n",
      "Overall rmse metric:  1.2693197344015619\n",
      "               tn\n",
      "20012  259.114394\n",
      "Fitting and predicting for product_id: 20007\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalidation's rmse: 3.30794\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalidation's rmse: 0.734613\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalidation's rmse: 0.944195\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalidation's rmse: 1.67617\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalidation's rmse: 0.747988\n",
      "Overall rmse metric:  0.5396562349746244\n",
      "               tn\n",
      "20007  419.022163\n",
      "Fitting and predicting for product_id: 20002\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalidation's rmse: 1.33487\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalidation's rmse: 10.3122\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalidation's rmse: 0.962165\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalidation's rmse: 6.2494\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalidation's rmse: 2.41234\n",
      "Overall rmse metric:  0.9257616577397979\n",
      "                tn\n",
      "20002  1038.471382\n",
      "Fitting and predicting for product_id: 20008\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalidation's rmse: 0.481188\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalidation's rmse: 0.619096\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalidation's rmse: 0.708258\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalidation's rmse: 0.270003\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalidation's rmse: 0.899636\n",
      "Overall rmse metric:  0.07290167440890982\n",
      "               tn\n",
      "20008  245.660579\n",
      "Fitting and predicting for product_id: 20006\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalidation's rmse: 0.515099\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalidation's rmse: 2.47284\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalidation's rmse: 0.588392\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalidation's rmse: 0.705971\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalidation's rmse: 0.570747\n",
      "Overall rmse metric:  0.265326980739605\n",
      "              tn\n",
      "20006  541.44168\n",
      "Fitting and predicting for product_id: 20004\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalidation's rmse: 0.949953\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalidation's rmse: 0.738957\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalidation's rmse: 0.380475\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalidation's rmse: 0.439791\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalidation's rmse: 0.462949\n",
      "Overall rmse metric:  0.1447610222522764\n",
      "               tn\n",
      "20004  601.443205\n",
      "Fitting and predicting for product_id: 20005\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalidation's rmse: 2.52251\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalidation's rmse: 2.60592\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalidation's rmse: 1.59485\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalidation's rmse: 2.19392\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalidation's rmse: 1.87155\n",
      "Overall rmse metric:  2.54354469220581\n",
      "               tn\n",
      "20005  512.001398\n",
      "Fitting and predicting for product_id: 20001\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalidation's rmse: 1.69001\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalidation's rmse: 1.0622\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalidation's rmse: 1.39492\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalidation's rmse: 1.46592\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalidation's rmse: 3.96059\n",
      "Overall rmse metric:  1.1282596736665793\n",
      "                tn\n",
      "20001  1564.534295\n",
      "Fitting and predicting for product_id: 20011\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's rmse: 1.87872\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalidation's rmse: 0.836575\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalidation's rmse: 1.28067\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalidation's rmse: 2.50511\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalidation's rmse: 1.08548\n",
      "Overall rmse metric:  0.6998568955692345\n",
      "               tn\n",
      "20011  394.470092\n",
      "Fitting and predicting for product_id: 20010\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalidation's rmse: 3.59519\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalidation's rmse: 1.11252\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalidation's rmse: 1.06523\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalidation's rmse: 1.33594\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\tvalidation's rmse: 1.40922\n",
      "Overall rmse metric:  1.1347131585554326\n",
      "               tn\n",
      "20010  396.891343\n",
      "Fitting and predicting for product_id: 20009\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's rmse: 2.52086\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalidation's rmse: 2.1138\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalidation's rmse: 2.65052\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalidation's rmse: 0.823559\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalidation's rmse: 0.467073\n",
      "Overall rmse metric:  0.21815719266946937\n",
      "               tn\n",
      "20009  486.365983\n"
     ]
    }
   ],
   "source": [
    "# Correr Modelo\n",
    "params={\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'Regression',\n",
    "        'metric':'rmse',\n",
    "        'verbose': -1,\n",
    "        #'n_jobs': -1,\n",
    "        'seed': 10000079,\n",
    "        #'learning_rate': 0.2,\n",
    "        #'bagging_fraction': 0.85,\n",
    "        #'bagging_freq': 1, \n",
    "        #'colsample_bytree': 0.85,\n",
    "        #'colsample_bynode': 0.85,\n",
    "        #'min_data_per_leaf': 25,\n",
    "        #'num_leaves': 200,\n",
    "        #'lambda_l1': 0.5,\n",
    "        #'lambda_l2': 0.5\n",
    "}\n",
    "\n",
    "predictions_all = pd.DataFrame(columns=['tn'])\n",
    "products = df_final['product_id'].unique()\n",
    "tot = len(products)\n",
    "nro = 0\n",
    "for producto in products:\n",
    "    print(f'Fitting and predicting for product_id: {producto}')\n",
    "    # Filtrar los datos del producto\n",
    "    df_producto = df_final[df_final['product_id'] == producto]\n",
    "    \n",
    "    product_weights = pd.merge(df_producto.reset_index(), weight, on=['product_id', 'customer_id'], how='left')\n",
    "    product_weights.set_index('periodo', inplace=True)\n",
    "    weight_p = product_weights['weight']\n",
    "   \n",
    "    #############################################################################  \n",
    "    #### Agrupar y escalar\n",
    "   \n",
    "    model, average_metric = labo.train_lightgbm_model(df_producto,params,metric='rmse',\n",
    "                                                      weights=weight_p)\n",
    "    print(\"Overall rmse metric: \", average_metric)\n",
    "    # Predict values for the entire dataset using the trained models\n",
    "    # Prepare last data points for prediction\n",
    "    df_predict_p = df_predict[df_predict['product_id'] == producto]\n",
    "    last_data_points = df_predict_p[df_predict_p.index == df_predict_p.index.max()].copy()\n",
    "    last_data_points.drop(columns=['tn_2'], inplace=True)\n",
    "    # Predict the next month's value using the trained model\n",
    "    predictions = labo.predict_next_month_customer(model, last_data_points)\n",
    "    preds = predictions.groupby(['product_id','customer_id'])['tn_2'].transform(inverse_scale_group)\n",
    "    predictions['tn'] = preds\n",
    "    predictions.drop(columns=['tn_2'], inplace=True)\n",
    "    predictions = predictions.reset_index()\n",
    "    predictions =  predictions.groupby('product_id')['tn'].sum()\n",
    "    predictions.columns = ['product_id', 'tn']\n",
    "    predictions_all = pd.concat([predictions_all, predictions])\n",
    "    print(predictions_all[-1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall custom metric:  0.21815719266946937\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_all['tn']=predictions_all['tn'].astype('float32')\n",
    "predictions_all.index.names = ['product_id']\n",
    "predictions_all.to_csv(DATOS_DIR+'/pred/0030-prediccion-RSME_scaled_Robust-product_id-Outliers.csv', index=True,header=True)\n",
    "print(\"Overall custom metric: \", average_metric)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
